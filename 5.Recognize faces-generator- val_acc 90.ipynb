{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Version 5.0\n",
    "# Manual loading images, image augmentation - image data generator\n",
    "# CHANGE: resizing images to 50%\n",
    "# CONCLUSIONS: half size images give 90-93% (probably to reason why it is better is not the size of image \n",
    "# but the filter settings - size/stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#all imports\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import csv\n",
    "import imageio\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD\n",
    "from keras import regularizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "K.set_image_dim_ordering('th')\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def rgb2gray(rgb):\n",
    "\n",
    "    r, g, b = rgb[:,:,0], rgb[:,:,1], rgb[:,:,2]\n",
    "    gray = 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
    "\n",
    "    return gray\n",
    "\n",
    "def readimagesfromfolder(path, color, show):\n",
    "\timagepackage_list = []\n",
    "\timagepackage = []\n",
    "\timagelist = []\n",
    "\tfilelist = sorted(os.listdir(path))\n",
    "\tfor filename in filelist:\n",
    "\t\tfullfilename =  path + filename\n",
    "\t\t#print(fullfilename)        \n",
    "\t\tsingleimage = imageio.imread(fullfilename)\n",
    "\t\tsingleimage = cv2.resize(singleimage,None, fx=0.5,fy=0.5, interpolation=cv2.INTER_AREA)        \n",
    "\t\tsingleimage = rgb2gray(singleimage)\n",
    "\t\t#print(singleimage)        \n",
    "\t\tif show==1:\n",
    "\t\t\tplt.imshow(singleimage)\n",
    "\t\t\tplt.show()\n",
    "\t\timagepackage.append(singleimage)\n",
    "\t\timagepackage_list.append(imagepackage)        \n",
    "\t\timagepackage = []        \n",
    "\t#print(len(imagepackage))        \n",
    "\timagepackage_list = np.float32(imagepackage_list)\n",
    "\t#cv2.destroyAllWindows()\n",
    "\treturn imagepackage_list\n",
    "\n",
    "def readlabelfromcsv(filename):\n",
    "    training_y = []\n",
    "    with open(filename) as f:\n",
    "        #training_y.append(f.readline())\n",
    "        content = csv.reader(f)\n",
    "        for line in content:\n",
    "            #print(line[0])\n",
    "            training_y.append(int(line[0]))\n",
    "    return training_y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(162, 1, 90, 90)\n",
      "(48, 1, 90, 90)\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "(162,)\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "(48,)\n"
     ]
    }
   ],
   "source": [
    "       \n",
    "folderpath = 'data/basic/training/'\n",
    "X_train=readimagesfromfolder(folderpath, 0, 0)\n",
    "print(np.shape(X_train))\n",
    "\n",
    "folderpath = 'data/basic/test/'\n",
    "X_test=readimagesfromfolder(folderpath, 0, 0)\n",
    "print(np.shape(X_test))\n",
    "\n",
    "y_train = readlabelfromcsv(\"data/basic/training.csv\")\n",
    "y_test = readlabelfromcsv(\"data/basic/test.csv\")\n",
    "\n",
    "print(y_train)\n",
    "print(np.shape(y_train))\n",
    "\n",
    "print(y_test)\n",
    "print(np.shape(y_test))\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "    samplewise_center=False,  # set each sample mean to 0\n",
    "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization=False,  # divide each input by its std\n",
    "    zca_whitening=False,  # apply ZCA whitening\n",
    "    rotation_range=8,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=True,  # randomly flip images\n",
    "    vertical_flip=False) # randomly flip images\n",
    "\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/keras/engine/training.py:1462: UserWarning: Epoch comprised more than `samples_per_epoch` samples, which might affect learning results. Set `samples_per_epoch` correctly to avoid this warning.\n",
      "  warnings.warn('Epoch comprised more than '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43s - loss: 0.7496 - acc: 0.6729 - val_loss: 0.4813 - val_acc: 0.8125\n",
      "Epoch 2/50\n",
      "13s - loss: 0.5125 - acc: 0.7968 - val_loss: 0.4711 - val_acc: 0.7708\n",
      "Epoch 3/50\n",
      "13s - loss: 0.3888 - acc: 0.8476 - val_loss: 0.5571 - val_acc: 0.8333\n",
      "Epoch 4/50\n",
      "13s - loss: 0.2623 - acc: 0.9031 - val_loss: 0.5691 - val_acc: 0.8333\n",
      "Epoch 5/50\n",
      "13s - loss: 0.2131 - acc: 0.9165 - val_loss: 0.4542 - val_acc: 0.8750\n",
      "Epoch 6/50\n",
      "13s - loss: 0.1365 - acc: 0.9519 - val_loss: 0.6546 - val_acc: 0.8750\n",
      "Epoch 7/50\n",
      "13s - loss: 0.1275 - acc: 0.9543 - val_loss: 0.5007 - val_acc: 0.8542\n",
      "Epoch 8/50\n",
      "13s - loss: 0.1002 - acc: 0.9620 - val_loss: 0.9451 - val_acc: 0.7917\n",
      "Epoch 9/50\n",
      "13s - loss: 0.0810 - acc: 0.9734 - val_loss: 0.6865 - val_acc: 0.8333\n",
      "Epoch 10/50\n",
      "13s - loss: 0.0671 - acc: 0.9758 - val_loss: 1.0214 - val_acc: 0.8333\n",
      "Epoch 11/50\n",
      "13s - loss: 0.0580 - acc: 0.9822 - val_loss: 0.7306 - val_acc: 0.8333\n",
      "Epoch 12/50\n",
      "13s - loss: 0.0725 - acc: 0.9768 - val_loss: 0.8167 - val_acc: 0.8333\n",
      "Epoch 13/50\n",
      "13s - loss: 0.0718 - acc: 0.9760 - val_loss: 0.5712 - val_acc: 0.8958\n",
      "Epoch 14/50\n",
      "13s - loss: 0.0394 - acc: 0.9856 - val_loss: 0.6488 - val_acc: 0.9375\n",
      "Epoch 15/50\n",
      "13s - loss: 0.0308 - acc: 0.9904 - val_loss: 0.8931 - val_acc: 0.8750\n",
      "Epoch 16/50\n",
      "13s - loss: 0.0301 - acc: 0.9886 - val_loss: 0.6593 - val_acc: 0.8333\n",
      "Epoch 17/50\n",
      "13s - loss: 0.0384 - acc: 0.9864 - val_loss: 0.8541 - val_acc: 0.8333\n",
      "Epoch 18/50\n",
      "13s - loss: 0.0446 - acc: 0.9854 - val_loss: 0.6145 - val_acc: 0.8958\n",
      "Epoch 19/50\n",
      "13s - loss: 0.0433 - acc: 0.9864 - val_loss: 0.4480 - val_acc: 0.9167\n",
      "Epoch 20/50\n",
      "13s - loss: 0.0314 - acc: 0.9904 - val_loss: 0.7094 - val_acc: 0.8750\n",
      "Epoch 21/50\n",
      "13s - loss: 0.0291 - acc: 0.9902 - val_loss: 0.5002 - val_acc: 0.9167\n",
      "Epoch 22/50\n",
      "13s - loss: 0.0253 - acc: 0.9916 - val_loss: 1.0884 - val_acc: 0.8125\n",
      "Epoch 23/50\n",
      "13s - loss: 0.0385 - acc: 0.9874 - val_loss: 0.8553 - val_acc: 0.8125\n",
      "Epoch 24/50\n",
      "13s - loss: 0.0272 - acc: 0.9906 - val_loss: 0.5494 - val_acc: 0.8958\n",
      "Epoch 25/50\n",
      "13s - loss: 0.0221 - acc: 0.9940 - val_loss: 0.6358 - val_acc: 0.8958\n",
      "Epoch 26/50\n",
      "13s - loss: 0.0179 - acc: 0.9954 - val_loss: 0.6891 - val_acc: 0.8750\n",
      "Epoch 27/50\n",
      "13s - loss: 0.0315 - acc: 0.9906 - val_loss: 0.6511 - val_acc: 0.8542\n",
      "Epoch 28/50\n",
      "13s - loss: 0.0253 - acc: 0.9906 - val_loss: 0.7257 - val_acc: 0.8542\n",
      "Epoch 29/50\n",
      "13s - loss: 0.0165 - acc: 0.9946 - val_loss: 0.7460 - val_acc: 0.8542\n",
      "Epoch 30/50\n",
      "13s - loss: 0.0289 - acc: 0.9904 - val_loss: 0.7123 - val_acc: 0.8750\n",
      "Epoch 31/50\n",
      "13s - loss: 0.0164 - acc: 0.9944 - val_loss: 0.5741 - val_acc: 0.8958\n",
      "Epoch 32/50\n",
      "13s - loss: 0.0508 - acc: 0.9880 - val_loss: 0.6952 - val_acc: 0.8542\n",
      "Epoch 33/50\n",
      "13s - loss: 0.0387 - acc: 0.9880 - val_loss: 0.5164 - val_acc: 0.9375\n",
      "Epoch 34/50\n",
      "13s - loss: 0.0345 - acc: 0.9880 - val_loss: 0.6100 - val_acc: 0.8333\n",
      "Epoch 35/50\n",
      "13s - loss: 0.0152 - acc: 0.9962 - val_loss: 0.6116 - val_acc: 0.8958\n",
      "Epoch 36/50\n",
      "13s - loss: 0.0195 - acc: 0.9938 - val_loss: 0.7046 - val_acc: 0.8958\n",
      "Epoch 37/50\n",
      "13s - loss: 0.0123 - acc: 0.9960 - val_loss: 0.8895 - val_acc: 0.8542\n",
      "Epoch 38/50\n",
      "13s - loss: 0.0117 - acc: 0.9962 - val_loss: 0.6070 - val_acc: 0.8958\n",
      "Epoch 39/50\n",
      "13s - loss: 0.0107 - acc: 0.9962 - val_loss: 0.8368 - val_acc: 0.8750\n",
      "Epoch 40/50\n",
      "13s - loss: 0.0165 - acc: 0.9960 - val_loss: 0.7131 - val_acc: 0.8333\n",
      "Epoch 41/50\n",
      "13s - loss: 0.0220 - acc: 0.9920 - val_loss: 0.8851 - val_acc: 0.8542\n",
      "Epoch 42/50\n",
      "13s - loss: 0.0119 - acc: 0.9956 - val_loss: 1.0122 - val_acc: 0.8542\n",
      "Epoch 43/50\n",
      "13s - loss: 0.0193 - acc: 0.9952 - val_loss: 0.9031 - val_acc: 0.8750\n",
      "Epoch 44/50\n",
      "13s - loss: 0.0217 - acc: 0.9934 - val_loss: 0.6837 - val_acc: 0.8750\n",
      "Epoch 45/50\n",
      "13s - loss: 0.0079 - acc: 0.9980 - val_loss: 0.6607 - val_acc: 0.8958\n",
      "Epoch 46/50\n",
      "13s - loss: 0.0211 - acc: 0.9942 - val_loss: 0.6993 - val_acc: 0.8750\n",
      "Epoch 47/50\n",
      "13s - loss: 0.0139 - acc: 0.9952 - val_loss: 0.8512 - val_acc: 0.8750\n",
      "Epoch 48/50\n",
      "13s - loss: 0.0175 - acc: 0.9942 - val_loss: 0.6324 - val_acc: 0.8958\n",
      "Epoch 49/50\n",
      "13s - loss: 0.0147 - acc: 0.9956 - val_loss: 0.5216 - val_acc: 0.9375\n",
      "Epoch 50/50\n",
      "13s - loss: 0.0103 - acc: 0.9964 - val_loss: 1.0166 - val_acc: 0.8750\n",
      "[1.0165709853172302, 0.875]\n",
      "Baseline Error: 12.50%\n"
     ]
    }
   ],
   "source": [
    "def larger_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Convolution2D(16, 3, 1, border_mode='same', input_shape=(1, 90, 90), activation='relu'))\n",
    "\tmodel.add(Convolution2D(24, 5, 3, border_mode='same', activation='relu'))\n",
    "\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\tmodel.add(Convolution2D(16, 3, 3, border_mode='same', activation='relu'))\n",
    "\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\tmodel.add(Dropout(0.5))\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(1000, activation='relu'))\n",
    "\tmodel.add(Dense(50, activation='relu'))\n",
    "\tmodel.add(Dense(num_classes, activation='softmax'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model\n",
    "\n",
    "# build the model\n",
    "model = larger_model()\n",
    "# Fit the model\n",
    "model.fit_generator(datagen.flow(X_train, y_train, batch_size=16),samples_per_epoch = 5000, nb_epoch = 50, verbose=2, validation_data=(X_test, y_test))\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print scores\n",
    "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
